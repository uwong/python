{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import make_scorer,  mean_absolute_error\n",
    "\n",
    "from scipy.stats.distributions import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define runtime parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_MODELS = True\n",
    "SAVE_FITTED_MODELS = False\n",
    "LOAD_FITTED_MODELS = False\n",
    "CREATE_SUBMISSION_CSV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = \"D:/data/kaggle/PUBG/\"\n",
    "MODEL_SAVE_DIRECTORY = WORKING_DIRECTORY + \"/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPreprocessingPipeline():\n",
    "    preprocPipeline = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True)\n",
    "    )\n",
    "    \n",
    "    return preprocPipeline\n",
    "\n",
    "def createModelPipeline(metricsScorer = None):\n",
    "    \n",
    "    paramDistributions = {\n",
    "        \"reg_alpha\": uniform(0.0, 1.0),\n",
    "        \"reg_lambda\": uniform(0.0, 1.0)\n",
    "    }\n",
    "    \n",
    "    numCores = os.cpu_count()\n",
    "    \n",
    "    print(\"number of CPU cores detected = %i\" % numCores)\n",
    "    \n",
    "    initParams =  {\n",
    "        \"max_depth\": 3,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"n_estimators\": 1000,\n",
    "        \"silent\": True,\n",
    "        \"objective\": 'reg:linear', \n",
    "        \"booster\":'gbtree', \n",
    "        \"n_jobs\": numCores, \n",
    "        \"gamma\" : 0, \n",
    "        \"min_child_weight\" : 1, \n",
    "        \"max_delta_step\": 0, \n",
    "        \"subsample\" : 1, \n",
    "        \"colsample_bytree\" : 1, \n",
    "        \"colsample_bylevel\":1, \n",
    "        \"reg_alpha\": 0.6975947598968077, \n",
    "        \"reg_lambda\": 0.14942377117732686,\n",
    "        \"scale_pos_weight\":1, \n",
    "        \"base_score\":0.5,\n",
    "#         \"tree_method\": \"gpu_hist\" \n",
    "    }\n",
    "    \n",
    "\n",
    "    modelPipeline = make_pipeline(\n",
    "        RandomizedSearchCV(\n",
    "            estimator = xgb.XGBRegressor(**initParams),\n",
    "            param_distributions = paramDistributions,\n",
    "            n_iter=10,\n",
    "            scoring = metricsScorer,\n",
    "            cv=3,\n",
    "            refit=True\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return modelPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataPath = WORKING_DIRECTORY + \"train.csv\"\n",
    "scoringDataPath = WORKING_DIRECTORY + \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv(trainingDataPath)\n",
    "scoringData = pd.read_csv(scoringDataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineerFeatures(data):\n",
    "    \n",
    "    engineered = data.copy()\n",
    "    \n",
    "    ###########################\n",
    "    # User level features\n",
    "    \n",
    "    ########################### \n",
    "    # Group level features\n",
    "    # number of players in group\n",
    "    # mean score of gropu\n",
    "    groupLevelFeatures = (\n",
    "        engineered.groupby([\"matchId\", \"groupId\"]).agg(\n",
    "            {\n",
    "                \"Id\": \"count\",\n",
    "                \"winPoints\": \"mean\"\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    groupLevelFeatures.rename(\n",
    "        columns={\n",
    "            \"Id\": \"numPlayersInGroup\",\n",
    "            \"winPoints\": \"meanGroupWinPoints\"\n",
    "        }, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    engineered = pd.merge(\n",
    "        engineered,\n",
    "        groupLevelFeatures,\n",
    "        on=[\"matchId\", \"groupId\"],\n",
    "        how=\"left\"\n",
    "        \n",
    "    )\n",
    "    \n",
    "    ###########################\n",
    "    # Match level features\n",
    "    \n",
    "    matchLevelFeatures = (\n",
    "        engineered.groupby([\"matchId\"]).agg(\n",
    "            {\n",
    "                \"Id\": \"count\",\n",
    "                \"winPoints\": \"mean\"\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    matchLevelFeatures.rename(\n",
    "        columns={\n",
    "            \"Id\": \"numPlayersInMatch\",\n",
    "            \"winPoints\": \"meanMatchWinPoints\"\n",
    "        }, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    engineered = pd.merge(\n",
    "        engineered,\n",
    "        matchLevelFeatures,\n",
    "        on=[\"matchId\"],\n",
    "        how=\"left\"\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_engineered = engineerFeatures(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "\n",
    "Need to do something smarter here:\n",
    "* split in away that respects matchid and groupIds\n",
    "\n",
    "Select by matchIds instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplitByMatchIds(df, matchIdFraction):\n",
    "\n",
    "    uniqueMatchIds = trainingData_engineered.matchId.unique()\n",
    "    numTrainingMatches = len(uniqueMatchIds)\n",
    "    print(\"Number of unique matches %i \" % numTrainingMatches)\n",
    "\n",
    "    sampleFraction = 0.7\n",
    "\n",
    "    trainMatchIds = np.random.choice(\n",
    "        uniqueMatchIds, \n",
    "        size = int(sampleFraction* numTrainingMatches), \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    len(trainMatchIds)\n",
    "\n",
    "    X_train = (\n",
    "        trainingData_engineered\n",
    "        .loc[trainingData_engineered.matchId.isin(trainMatchIds)]\n",
    "        .drop(\"winPlacePerc\", axis=\"columns\")\n",
    "    )\n",
    "\n",
    "    y_train = (\n",
    "        trainingData_engineered\n",
    "        .loc[trainingData_engineered.matchId.isin(trainMatchIds), \"winPlacePerc\"]\n",
    "    )\n",
    "\n",
    "    X_test = (\n",
    "        trainingData_engineered\n",
    "        .loc[~trainingData_engineered.matchId.isin(trainMatchIds)]\n",
    "        .drop(\"winPlacePerc\", axis=\"columns\")\n",
    "    )\n",
    "\n",
    "    y_test = (\n",
    "        trainingData_engineered\n",
    "        .loc[~trainingData_engineered.matchId.isin(trainMatchIds), \"winPlacePerc\"]\n",
    "    )\n",
    "\n",
    "    print(\"Train X, y sizes \", X_train.shape, y_train.shape)\n",
    "    print(\"Test X, y sizes \", X_test.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique matches 47734 \n",
      "Train X, y sizes  (3049858, 29) (3049858,)\n",
      "Test X, y sizes  (1307478, 29) (1307478,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = trainTestSplitByMatchIds(trainingData_engineered, matchIdFraction=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = createPreprocessingPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of CPU cores detected = 8\n"
     ]
    }
   ],
   "source": [
    "xgbPipeline = createModelPipeline(\n",
    "    metricsScorer =make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('randomizedsearchcv', RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimat...\n",
       "          scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
       "          verbose=0))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPipelines(preprocessingPipeline, modelPipeline, selectedFeatures, X, y, Xtest, ytest):\n",
    "    \n",
    "    # run preprocessing pipeline on raw data\n",
    "    print(\"Fit preprocessing pipeline\")\n",
    "    fittedPreprocessingPipeline = preprocessingPipeline.fit(X.loc[:, selectedFeatures], y)\n",
    "    \n",
    "    print(\"Preprocess data\")\n",
    "    X_processed = fittedPreprocessingPipeline.transform(X.loc[:, selectedFeatures])\n",
    "    Xtest_processed = fittedPreprocessingPipeline.transform(Xtest.loc[:, selectedFeatures])\n",
    "    \n",
    "    eval_set = [\n",
    "        (X_processed, y), \n",
    "        (Xtest_processed, ytest)\n",
    "    ]\n",
    "    \n",
    "    print(\"Fit model pipeline\")\n",
    "    # run the model fitting pipeline on the processed data\n",
    "    fittedModelPipeline = modelPipeline.fit(\n",
    "        X_processed, \n",
    "        y,\n",
    "        # xgb options\n",
    "        randomizedsearchcv__verbose=0,\n",
    "        randomizedsearchcv__eval_metric=\"mae\",\n",
    "        randomizedsearchcv__eval_set = eval_set,\n",
    "        randomizedsearchcv__early_stopping_rounds=5\n",
    "    )\n",
    "\n",
    "    # Print out some common eval metrics\n",
    "    cvResults = fittedModelPipeline.named_steps.get('randomizedsearchcv')\n",
    "    \n",
    "    print(\"best estimator\")\n",
    "    print(cvResults.best_estimator_)\n",
    "    \n",
    "    print(\"best score\")\n",
    "    print(cvResults.best_score_)\n",
    "    \n",
    "    print(\"early stopping\")\n",
    "    print(\"num trees = %i\" % cvResults.best_estimator_.best_ntree_limit)\n",
    "    print(\"best score = %i\" % cvResults.best_estimator_.best_score)\n",
    "    print(\"best iteration = %i\" % cvResults.best_estimator_.best_iteration)\n",
    "    \n",
    "    \n",
    "    # return the fitted preprocessing and modeling pipelines\n",
    "    return fittedPreprocessingPipeline, fittedModelPipeline\n",
    "\n",
    "\n",
    "def evalModel(y_true, y_pred, metric_function):\n",
    "        \n",
    "    result = metric_function(y_true, y_pred)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(y_pred, y_true, alpha = 0.01)\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Ground truth\")\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                      int64\n",
       "groupId                 int64\n",
       "matchId                 int64\n",
       "assists                 int64\n",
       "boosts                  int64\n",
       "damageDealt           float64\n",
       "DBNOs                   int64\n",
       "headshotKills           int64\n",
       "heals                   int64\n",
       "killPlace               int64\n",
       "killPoints              int64\n",
       "kills                   int64\n",
       "killStreaks             int64\n",
       "longestKill           float64\n",
       "maxPlace                int64\n",
       "numGroups               int64\n",
       "revives                 int64\n",
       "rideDistance          float64\n",
       "roadKills               int64\n",
       "swimDistance          float64\n",
       "teamKills               int64\n",
       "vehicleDestroys         int64\n",
       "walkDistance          float64\n",
       "weaponsAcquired         int64\n",
       "winPoints               int64\n",
       "meanGroupWinPoints    float64\n",
       "numPlayersInGroup       int64\n",
       "meanMatchWinPoints    float64\n",
       "numPlayersInMatch       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedFeatures = [\n",
    "    'assists', 'boosts', 'damageDealt', 'DBNOs',\n",
    "       'headshotKills', 'heals', 'killPlace', 'killPoints', 'kills',\n",
    "       'killStreaks', 'longestKill', 'maxPlace', 'numGroups', 'revives',\n",
    "       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n",
    "       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'winPoints',\n",
    "        \"numPlayersInGroup\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 3049858, Test size = 1307478\n"
     ]
    }
   ],
   "source": [
    "nRows_train = X_train.shape[0]\n",
    "nRows_test = X_test.shape[0]\n",
    "\n",
    "print(\"Train size = %i, Test size = %i\" % (nRows_train, nRows_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleTrainSize = 200000\n",
    "# sampleTestSize = 100000\n",
    "sampleTrainSize = 2000\n",
    "sampleTestSize = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit preprocessing pipeline\n",
      "Preprocess data\n",
      "Fit model pipeline\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if FIT_MODELS:\n",
    "#     fittedPreprocessing, fittedModelPipeline = fitPipelines(\n",
    "    fitPipelines(\n",
    "        preprocessingPipeline=preprocessingPipeline,\n",
    "        modelPipeline=xgbPipeline, \n",
    "        selectedFeatures=selectedFeatures, \n",
    "        X=X_train[:sampleTrainSize], \n",
    "        y=y_train[:sampleTrainSize],\n",
    "        Xtest=X_test[:sampleTestSize],\n",
    "        ytest=y_test[:sampleTestSize]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedModelPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_FITTED_MODELS:\n",
    "    joblib.dump(fittedPreprocessing, \"./models/preprocessing.pkl\")\n",
    "    joblib.dump(fittedModelPipeline, \"./models/xgbModelPipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_FITTED_MODELS:\n",
    "    fittedPreprocessing = joblib.load(\"./models/preprocessing.pkl\")\n",
    "    fittedModelPipeline = joblib.load(\"./models/xgbModelPipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedXgb = fittedModelPipeline.named_steps.get(\"randomizedsearchcv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportances = pd.Series(fittedXgb.best_estimator_.feature_importances_, index=selectedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictScores(fittedPreprocessingPipeline, fittedModelPipeline, selectedFeatures, X):\n",
    "    \n",
    "    X_processed = fittedPreprocessingPipeline.transform(X.loc[:, selectedFeatures])\n",
    "    \n",
    "    y_preds = fittedModelPipeline.predict(X_processed)\n",
    "\n",
    "    return y_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_user_preds = predictScores(fittedPreprocessing, fittedModelPipeline, selectedFeatures, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateGroupScores(X, y_predictions):\n",
    "    \n",
    "    userPreds = X.copy()\n",
    "    \n",
    "    userPreds[\"user_prediction\"] = y_predictions\n",
    "    \n",
    "    ################\n",
    "    # Within each match players in the same group have the same score\n",
    "    \n",
    "#     aux = userPreds.groupby(['matchId','groupId'])['prediction'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\n",
    "#     return aux\n",
    "    \n",
    "    # get the mean score for users in the group\n",
    "    groupMeanScores = (\n",
    "        userPreds\n",
    "        .groupby([\"matchId\", \"groupId\"])[\"user_prediction\"]\n",
    "        .agg(\"mean\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    # Within the match rank the scores by group\n",
    "    matchPctRanks = (\n",
    "        groupMeanScores\n",
    "        .groupby([\"matchId\"])\n",
    "        .rank(pct=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    matchPctRanks.columns=[\"matchId\", \"groupId\", \"group_prediction\"]\n",
    "    \n",
    "    # Merge the process scores back into the user data\n",
    "    userGroupPctRankedScores = pd.merge(\n",
    "        userPreds,\n",
    "        matchPctRanks, \n",
    "        on = [\"matchId\", \"groupId\"],\n",
    "        how = \"left\"\n",
    "    )\n",
    "    \n",
    "    return userGroupPctRankedScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData = estimateGroupScores(X_test, y_user_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_user_preds.shape, groupData.group_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user vs group predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(groupData.group_prediction, y_user_preds, alpha =0.1)\n",
    "plt.xlabel(\"group prediction\")\n",
    "plt.ylabel(\"user prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData.group_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData.group_prediction.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_user_preds_clipped = np.clip(y_user_preds, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalModel(y_test, y_user_preds, mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalModel(y_test, y_user_preds_clipped, mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group scores look worse\n",
    "\n",
    "something about our sampling method?\n",
    "\n",
    "Are some of the groups arbitrarily split between train and test data?\n",
    "\n",
    "yes...\n",
    "\n",
    "need to do this for all training data to prevent group splits\n",
    "\n",
    "Assumes scoring data will have complete groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalModel(y_test, groupData.group_prediction, mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the group where we are predicting 1.0 when it is 0.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData[\"ground_truth\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = (\n",
    "    groupData\n",
    "    .loc[\n",
    "        (groupData.ground_truth == 0) &\\\n",
    "        (groupData.group_prediction > 0.99),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.loc[:, selectedFeatures].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData.loc[:, selectedFeatures].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(selectedFeatures)), outliers.loc[:, selectedFeatures].mean(), label=\"outliers\")\n",
    "plt.bar(range(len(selectedFeatures)), groupData.loc[:, selectedFeatures].mean(), label=\"all\")\n",
    "plt.xticks(range(len(selectedFeatures)), selectedFeatures, rotation=90)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(selectedFeatures)), groupData.loc[:, selectedFeatures].mean())\n",
    "plt.xticks(range(len(selectedFeatures)), selectedFeatures, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoringData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoringPreds = engineerFeatures(scoringData)\n",
    "\n",
    "# user scores\n",
    "scoringPreds[\"userPredictions\"] = predictScores(fittedPreprocessing, fittedModelPipeline, selectedFeatures, scoringPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoringGroupPreds = estimateGroupScores(scoringData, scoringPreds[\"userPredictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    scoringGroupPreds\n",
    "    .loc[:, [\"Id\", \"user_prediction\", \"group_prediction\"]]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_SUBMISSION_CSV:\n",
    "    submission = (\n",
    "        scoringGroupPreds\n",
    "        .rename(columns={\"group_prediction\": \"winPlacePerc\"})\n",
    "        .loc[:, [\"Id\", \"winPlacePerc\"]]\n",
    "    )\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
